{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.3 Linear Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/battlerhythm/tensorflow/blob/master/3.3%20Linear%20Regression.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "83IOa21IQvuZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1, 2, 3]\n",
        "y_data = [1, 2, 3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bI8_OZAzRAYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
        "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-knDm2NdRGjh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, name=\"X\")\n",
        "Y = tf.placeholder(tf.float32, name=\"Y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxeOcydjRQuj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# W와 X가 행렬이 아니므로 tf.matmul이 아니라 기본 곱셈 연산자를 사용했습니다.\n",
        "hypothesis = W * X + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8OYFUGkRa-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 손실함수(Loss Function)\n",
        "# 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값입니다.\n",
        "# 손실을 전체 데이터에 대해 구한 경우 이를 비용(Cost)라고 합니다.\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoM0jpZfRjtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 최적화 함수의 매개변수인 learning_rate, 즉 학습률은 학습을 얼마나\n",
        "# '급하게'할 것인가를 설정하는 값입니다. 값이 너무 크면 최적의 손실값을 찾지 못하고\n",
        "# 지나치게 되고, 값이 너무 작으면 학습 속도가 매우 느려집니다. 이렇게 학습을\n",
        "# 진행하는 과정에 영향을 주는 변수를 하이퍼파라미터(Hyperparameter)라 합니다.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train_op = optimizer.minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NVvhPhQyRwgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        },
        "outputId": "26b33bc2-83e8-4883-ec6d-0f9f9c1dde4b"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # 최적화를 100번 수행합니다.\n",
        "    for step in range(100):\n",
        "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
        "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
        "        # cost_val = sess.run(cost, feed_dict={X: x_data, Y: y_data})\n",
        "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        \n",
        "        print(step, cost_val, sess.run(W), sess.run(b))\n",
        "\n",
        "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
        "    print(\"\\n=== Test ===\")\n",
        "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
        "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n",
        "    \n",
        "    sess.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 16.715822 [0.77643114] [0.9777013]\n",
            "1 0.3148198 [0.5940148] [0.8715886]\n",
            "2 0.11343694 [0.6242989] [0.8596649]\n",
            "3 0.10582166 [0.6310873] [0.8380124]\n",
            "4 0.1007685 [0.64020085] [0.817975]\n",
            "5 0.095981605 [0.6488234] [0.7982996]\n",
            "6 0.09142239 [0.65726835] [0.7791103]\n",
            "7 0.08707974 [0.6655071] [0.7603809]\n",
            "8 0.08294342 [0.6735481] [0.7421019]\n",
            "9 0.07900355 [0.6813958] [0.7242623]\n",
            "10 0.075250804 [0.6890548] [0.7068515]\n",
            "11 0.0716763 [0.69652975] [0.6898593]\n",
            "12 0.068271644 [0.70382494] [0.67327553]\n",
            "13 0.06502872 [0.7109448] [0.6570904]\n",
            "14 0.061939765 [0.7178935] [0.6412944]\n",
            "15 0.058997598 [0.7246751] [0.62587816]\n",
            "16 0.05619519 [0.73129374] [0.61083245]\n",
            "17 0.05352584 [0.7377533] [0.59614843]\n",
            "18 0.05098333 [0.74405754] [0.58181745]\n",
            "19 0.048561618 [0.7502102] [0.567831]\n",
            "20 0.046254877 [0.756215] [0.5541807]\n",
            "21 0.044057768 [0.7620754] [0.54085857]\n",
            "22 0.041964952 [0.76779497] [0.5278567]\n",
            "23 0.03997159 [0.773377] [0.5151674]\n",
            "24 0.038072903 [0.7788248] [0.5027831]\n",
            "25 0.03626442 [0.7841418] [0.49069658]\n",
            "26 0.034541827 [0.7893308] [0.47890055]\n",
            "27 0.032901097 [0.79439515] [0.46738812]\n",
            "28 0.031338245 [0.7993378] [0.45615244]\n",
            "29 0.02984968 [0.80416155] [0.44518682]\n",
            "30 0.028431782 [0.80886936] [0.43448484]\n",
            "31 0.027081242 [0.813464] [0.4240401]\n",
            "32 0.025794873 [0.8179482] [0.4138465]\n",
            "33 0.024569603 [0.82232463] [0.4038979]\n",
            "34 0.02340252 [0.8265958] [0.39418846]\n",
            "35 0.022290876 [0.83076435] [0.38471246]\n",
            "36 0.021232039 [0.8348326] [0.37546423]\n",
            "37 0.020223526 [0.8388032] [0.36643833]\n",
            "38 0.019262874 [0.84267825] [0.3576294]\n",
            "39 0.018347884 [0.84646016] [0.34903222]\n",
            "40 0.017476339 [0.8501511] [0.3406417]\n",
            "41 0.016646186 [0.8537534] [0.33245292]\n",
            "42 0.015855484 [0.85726905] [0.32446098]\n",
            "43 0.015102345 [0.8607002] [0.31666118]\n",
            "44 0.014384975 [0.86404884] [0.30904886]\n",
            "45 0.013701688 [0.8673171] [0.30161956]\n",
            "46 0.013050842 [0.8705067] [0.29436883]\n",
            "47 0.012430918 [0.8736196] [0.2872924]\n",
            "48 0.011840428 [0.87665766] [0.28038606]\n",
            "49 0.011278011 [0.87962276] [0.2736458]\n",
            "50 0.010742284 [0.8825165] [0.26706752]\n",
            "51 0.010232025 [0.88534075] [0.26064742]\n",
            "52 0.009745993 [0.88809705] [0.25438163]\n",
            "53 0.009283057 [0.8907871] [0.24826647]\n",
            "54 0.008842102 [0.89341253] [0.24229833]\n",
            "55 0.008422095 [0.8959749] [0.23647366]\n",
            "56 0.008022047 [0.8984756] [0.23078899]\n",
            "57 0.007640995 [0.9009161] [0.22524096]\n",
            "58 0.0072780345 [0.903298] [0.21982633]\n",
            "59 0.006932322 [0.90562266] [0.21454185]\n",
            "60 0.0066030324 [0.90789145] [0.20938443]\n",
            "61 0.006289381 [0.9101057] [0.20435098]\n",
            "62 0.0059906323 [0.9122667] [0.19943851]\n",
            "63 0.0057060733 [0.9143757] [0.19464414]\n",
            "64 0.005435031 [0.91643405] [0.18996502]\n",
            "65 0.0051768627 [0.9184429] [0.1853984]\n",
            "66 0.004930965 [0.92040354] [0.18094157]\n",
            "67 0.0046967273 [0.92231697] [0.17659184]\n",
            "68 0.004473634 [0.92418444] [0.1723467]\n",
            "69 0.0042611337 [0.926007] [0.16820359]\n",
            "70 0.004058732 [0.9277857] [0.16416009]\n",
            "71 0.0038659351 [0.9295217] [0.1602138]\n",
            "72 0.0036823004 [0.93121594] [0.15636237]\n",
            "73 0.0035073925 [0.9328695] [0.15260352]\n",
            "74 0.0033407814 [0.9344832] [0.148935]\n",
            "75 0.003182087 [0.93605816] [0.14535472]\n",
            "76 0.0030309402 [0.9375953] [0.1418605]\n",
            "77 0.0028869768 [0.9390955] [0.13845026]\n",
            "78 0.002749839 [0.9405596] [0.13512203]\n",
            "79 0.002619219 [0.94198847] [0.13187377]\n",
            "80 0.0024948 [0.94338304] [0.12870362]\n",
            "81 0.0023763 [0.94474405] [0.12560968]\n",
            "82 0.0022634275 [0.9460724] [0.12259012]\n",
            "83 0.0021559093 [0.94736874] [0.11964314]\n",
            "84 0.0020534957 [0.94863397] [0.116767]\n",
            "85 0.001955959 [0.9498688] [0.11396001]\n",
            "86 0.0018630469 [0.9510739] [0.11122047]\n",
            "87 0.0017745551 [0.9522501] [0.10854684]\n",
            "88 0.0016902596 [0.9533979] [0.10593743]\n",
            "89 0.0016099712 [0.95451826] [0.10339078]\n",
            "90 0.0015334993 [0.9556116] [0.10090532]\n",
            "91 0.001460655 [0.9566786] [0.09847962]\n",
            "92 0.0013912743 [0.9577201] [0.09611226]\n",
            "93 0.0013251876 [0.9587364] [0.09380175]\n",
            "94 0.0012622381 [0.9597284] [0.09154683]\n",
            "95 0.0012022838 [0.9606965] [0.0893461]\n",
            "96 0.001145169 [0.9616413] [0.08719827]\n",
            "97 0.0010907749 [0.96256346] [0.0851021]\n",
            "98 0.0010389661 [0.96346337] [0.08305629]\n",
            "99 0.0009896111 [0.9643417] [0.08105968]\n",
            "\n",
            "=== Test ===\n",
            "X: 5, Y: [4.902768]\n",
            "X: 2.5, Y: [2.491914]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}