{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.3 Linear Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/battlerhythm/tensorflow/blob/master/3.3%20Linear%20Regression.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "83IOa21IQvuZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x_data = [1, 2, 3]\n",
        "y_data = [1, 2, 3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bI8_OZAzRAYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
        "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-knDm2NdRGjh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, name=\"X\")\n",
        "Y = tf.placeholder(tf.float32, name=\"Y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxeOcydjRQuj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# W와 X가 행렬이 아니므로 tf.matmul이 아니라 기본 곱셈 연산자를 사용했습니다.\n",
        "hypothesis = W * X + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8OYFUGkRa-X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 손실함수(Loss Function)\n",
        "# 실제값과 모델로 예측한 값이 얼마나 차이가 나는가를 나타내는 값입니다.\n",
        "# 손실을 전체 데이터에 대해 구한 경우 이를 비용(Cost)라고 합니다.\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NoM0jpZfRjtM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 최적화 함수의 매개변수인 learning_rate, 즉 학습률은 학습을 얼마나\n",
        "# '급하게'할 것인가를 설정하는 값입니다. 값이 너무 크면 최적의 손실값을 찾지 못하고\n",
        "# 지나치게 되고, 값이 너무 작으면 학습 속도가 매우 느려집니다. 이렇게 학습을\n",
        "# 진행하는 과정에 영향을 주는 변수를 하이퍼파라미터(Hyperparameter)라 합니다.\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train_op = optimizer.minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NVvhPhQyRwgr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1785
        },
        "outputId": "d468e28b-770f-4db3-f9a8-4bec292f095a"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # 최적화를 100번 수행합니다.\n",
        "    for step in range(100):\n",
        "        # sess.run 을 통해 train_op 와 cost 그래프를 계산합니다.\n",
        "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달합니다.\n",
        "        # cost_val = sess.run(cost, feed_dict={X: x_data, Y: y_data})\n",
        "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
        "        \n",
        "        print(step, cost_val, sess.run(W), sess.run(b))\n",
        "\n",
        "# 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봅니다.\n",
        "    print(\"\\n=== Test ===\")\n",
        "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
        "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 9.796718 [0.9021394] [0.582431]\n",
            "1 0.15592904 [0.76050353] [0.50508904]\n",
            "2 0.038920045 [0.781998] [0.49986982]\n",
            "3 0.035762068 [0.7855186] [0.48709667]\n",
            "4 0.034047734 [0.7908626] [0.47546992]\n",
            "5 0.03243025 [0.7958695] [0.4640309]\n",
            "6 0.030889787 [0.8007789] [0.4528769]\n",
            "7 0.0294225 [0.80556786] [0.44198996]\n",
            "8 0.02802491 [0.8102419] [0.4313648]\n",
            "9 0.026693719 [0.81480354] [0.4209951]\n",
            "10 0.025425741 [0.8192556] [0.41087466]\n",
            "11 0.024217984 [0.82360053] [0.40099752]\n",
            "12 0.02306763 [0.82784104] [0.3913578]\n",
            "13 0.021971881 [0.8319796] [0.3819498]\n",
            "14 0.020928198 [0.8360187] [0.372768]\n",
            "15 0.019934103 [0.8399607] [0.36380693]\n",
            "16 0.018987212 [0.8438079] [0.35506126]\n",
            "17 0.01808531 [0.84756273] [0.34652588]\n",
            "18 0.017226243 [0.85122716] [0.33819562]\n",
            "19 0.016408008 [0.8548036] [0.33006564]\n",
            "20 0.01562859 [0.85829395] [0.32213104]\n",
            "21 0.014886231 [0.86170053] [0.31438726]\n",
            "22 0.0141791245 [0.8650251] [0.30682957]\n",
            "23 0.013505596 [0.8682698] [0.2994536]\n",
            "24 0.012864088 [0.8714366] [0.29225495]\n",
            "25 0.012253016 [0.87452716] [0.28522933]\n",
            "26 0.0116709955 [0.87754345] [0.27837262]\n",
            "27 0.011116616 [0.8804872] [0.2716807]\n",
            "28 0.010588571 [0.8833602] [0.26514968]\n",
            "29 0.010085597 [0.8861641] [0.25877565]\n",
            "30 0.009606522 [0.8889007] [0.25255486]\n",
            "31 0.009150202 [0.89157146] [0.24648362]\n",
            "32 0.008715572 [0.894178] [0.24055831]\n",
            "33 0.008301563 [0.89672184] [0.23477545]\n",
            "34 0.007907233 [0.89920455] [0.22913161]\n",
            "35 0.0075316336 [0.90162766] [0.22362345]\n",
            "36 0.007173868 [0.9039924] [0.21824768]\n",
            "37 0.006833114 [0.9063004] [0.21300119]\n",
            "38 0.006508537 [0.9085529] [0.20788078]\n",
            "39 0.006199375 [0.9107512] [0.20288348]\n",
            "40 0.005904908 [0.9128967] [0.1980063]\n",
            "41 0.005624411 [0.91499054] [0.19324635]\n",
            "42 0.0053572482 [0.91703415] [0.18860085]\n",
            "43 0.005102784 [0.91902864] [0.18406703]\n",
            "44 0.0048603965 [0.92097515] [0.17964217]\n",
            "45 0.0046295174 [0.9228748] [0.17532368]\n",
            "46 0.00440962 [0.9247289] [0.17110902]\n",
            "47 0.0042001507 [0.92653835] [0.16699567]\n",
            "48 0.0040006433 [0.92830426] [0.1629812]\n",
            "49 0.0038106057 [0.9300278] [0.15906325]\n",
            "50 0.0036295995 [0.9317099] [0.15523948]\n",
            "51 0.0034571874 [0.93335146] [0.1515076]\n",
            "52 0.003292973 [0.93495375] [0.1478655]\n",
            "53 0.0031365503 [0.93651736] [0.1443109]\n",
            "54 0.0029875657 [0.9380434] [0.14084177]\n",
            "55 0.0028456573 [0.9395328] [0.13745604]\n",
            "56 0.0027104851 [0.94098645] [0.13415171]\n",
            "57 0.002581738 [0.94240505] [0.13092677]\n",
            "58 0.002459098 [0.9437896] [0.1277794]\n",
            "59 0.0023422863 [0.94514084] [0.12470765]\n",
            "60 0.0022310333 [0.94645965] [0.12170978]\n",
            "61 0.0021250604 [0.94774675] [0.11878396]\n",
            "62 0.0020241137 [0.94900286] [0.11592846]\n",
            "63 0.0019279626 [0.9502288] [0.11314163]\n",
            "64 0.0018363866 [0.95142525] [0.11042178]\n",
            "65 0.0017491593 [0.952593] [0.10776734]\n",
            "66 0.0016660691 [0.9537326] [0.10517667]\n",
            "67 0.0015869294 [0.95484483] [0.10264828]\n",
            "68 0.001511553 [0.95593035] [0.1001807]\n",
            "69 0.0014397515 [0.95698977] [0.09777242]\n",
            "70 0.0013713605 [0.9580237] [0.09542204]\n",
            "71 0.001306225 [0.9590328] [0.09312814]\n",
            "72 0.0012441716 [0.96001756] [0.09088939]\n",
            "73 0.0011850749 [0.96097875] [0.0887045]\n",
            "74 0.0011287815 [0.96191674] [0.08657209]\n",
            "75 0.0010751635 [0.9628323] [0.08449096]\n",
            "76 0.0010240916 [0.96372575] [0.08245986]\n",
            "77 0.00097544893 [0.96459776] [0.08047759]\n",
            "78 0.0009291141 [0.96544886] [0.07854297]\n",
            "79 0.0008849804 [0.9662794] [0.07665484]\n",
            "80 0.00084294373 [0.96709] [0.07481211]\n",
            "81 0.000802903 [0.96788114] [0.07301369]\n",
            "82 0.0007647653 [0.96865326] [0.07125849]\n",
            "83 0.0007284359 [0.9694068] [0.06954549]\n",
            "84 0.000693836 [0.9701423] [0.06787369]\n",
            "85 0.0006608783 [0.97086] [0.06624204]\n",
            "86 0.0006294861 [0.97156054] [0.06464964]\n",
            "87 0.00059958635 [0.97224414] [0.0630955]\n",
            "88 0.0005711054 [0.9729114] [0.06157874]\n",
            "89 0.0005439749 [0.9735626] [0.06009843]\n",
            "90 0.0005181395 [0.97419816] [0.05865372]\n",
            "91 0.0004935253 [0.97481835] [0.0572437]\n",
            "92 0.0004700831 [0.97542375] [0.05586762]\n",
            "93 0.00044775382 [0.97601455] [0.0545246]\n",
            "94 0.00042648523 [0.9765911] [0.05321386]\n",
            "95 0.00040622693 [0.9771539] [0.05193465]\n",
            "96 0.0003869283 [0.97770303] [0.05068615]\n",
            "97 0.00036855348 [0.97823906] [0.04946771]\n",
            "98 0.00035104415 [0.97876215] [0.04827853]\n",
            "99 0.00033437085 [0.9792727] [0.04711796]\n",
            "\n",
            "=== Test ===\n",
            "X: 5, Y: [4.943482]\n",
            "X: 2.5, Y: [2.4952998]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}